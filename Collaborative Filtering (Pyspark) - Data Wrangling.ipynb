{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aisles = spark.read \\\n",
    "    .options(header=True, inferSchema=True) \\\n",
    "    .csv(\"Instacart Kaggle/aisles.csv\")\n",
    "dptmts = sqlContext.read \\\n",
    "    .options(header=True, inferSchema=True) \\\n",
    "    .csv(\"Instacart Kaggle/departments.csv\")\n",
    "prod_in_orders = sqlContext.read \\\n",
    "    .options(header=True, inferSchema=True) \\\n",
    "    .csv(\"Instacart Kaggle/order_products__prior.csv\")\n",
    "all_orders = sqlContext.read \\\n",
    "    .options(header=True, inferSchema=True) \\\n",
    "    .csv(\"Instacart Kaggle/orders.csv\")\n",
    "train = sqlContext.read \\\n",
    "    .options(header=True, inferSchema=True) \\\n",
    "    .csv(\"Instacart Kaggle/order_products__train.csv\")\n",
    "products = sqlContext.read \\\n",
    "    .options(header=True, inferSchema=True) \\\n",
    "    .csv(\"Instacart Kaggle/products.csv\")\n",
    "sample_sub = sqlContext.read \\\n",
    "    .options(header=True, inferSchema=True) \\\n",
    "    .csv(\"Instacart Kaggle/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "\n",
    "windowval = Window.partitionBy('user_id').orderBy('order_number').rangeBetween(Window.unboundedPreceding, 0)\n",
    "windowval2 = Window.partitionBy('user_id', 'product_id').orderBy('order_number')\n",
    "all_orders = all_orders.withColumn('dspo_cum_sum', F.sum('days_since_prior_order').over(windowval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The first join will connect the products tables\n",
    "prod_full = products.join(aisles, on='aisle_id')\n",
    "prod_full = prod_full.join(dptmts, on='department_id')\n",
    "#The second join will connect the orders tables\n",
    "order_full = prod_in_orders.join(all_orders, on='order_id')\n",
    "# The third join will connect products and orders\n",
    "order_prod_full = order_full.join(prod_full, on='product_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "opf = order_prod_full\n",
    "windowval = Window.partitionBy('user_id','product_id').orderBy('order_number').rangeBetween(Window.unboundedPreceding, 0)\n",
    "opf = opf.withColumn('MIN_dspo_cum_sum', F.min('dspo_cum_sum').over(windowval))\n",
    "opf = opf.withColumn('MAX_dspo_cum_sum', F.max('dspo_cum_sum').over(windowval))\n",
    "opf = opf.withColumn('COUNT_dspo_cum_sum', F.count('dspo_cum_sum').over(windowval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opf = opf.withColumn('SND_MAX_dspo_cum_sum', F.lag('dspo_cum_sum',1,0).over(windowval2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opf = opf.withColumn('DIFF_dspo_cum_sum', opf.MAX_dspo_cum_sum - opf.MIN_dspo_cum_sum)\n",
    "opf = opf.withColumn('DIFF2_dspo_cum_sum', opf.MAX_dspo_cum_sum - opf.SND_MAX_dspo_cum_sum)\n",
    "opf = opf.withColumn('fixed_freq', opf.DIFF_dspo_cum_sum / (opf.COUNT_dspo_cum_sum - 1))\n",
    "opf = opf.withColumn('flex_freq_1', opf.DIFF2_dspo_cum_sum / (opf.COUNT_dspo_cum_sum - 1))\n",
    "opf = opf.withColumn('flex_freq_2', opf.DIFF2_dspo_cum_sum / (opf.COUNT_dspo_cum_sum - 1))\n",
    "opf = opf.withColumnRenamed('COUNT_dspo_cum_sum', 'count_user_prod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opf = opf.drop('order_id', 'add_to_cart_order', 'reordered', 'eval_set', 'order_dow', 'order_hour_of_day', 'department_id', 'aisle_id', 'product_name', 'aisle', 'department')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opf2 = opf.groupby(['user_id', 'product_id']).agg({'count_user_prod': 'max', 'flex_freq_1': 'mean', 'flex_freq_2': 'stddev_pop'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------------------+------------------+-----------------------+\n",
      "|user_id|product_id|max(count_user_prod)|  avg(flex_freq_1)|stddev_pop(flex_freq_2)|\n",
      "+-------+----------+--------------------+------------------+-----------------------+\n",
      "|      7|     29894|                   1|              null|                   null|\n",
      "|      7|     29993|                   3|              41.5|                    9.5|\n",
      "|     14|     23271|                   1|              null|                   null|\n",
      "|     14|     34234|                   1|              null|                   null|\n",
      "|     31|     28577|                   2|              26.0|                    0.0|\n",
      "|     31|     34234|                   1|              null|                   null|\n",
      "|     37|     31528|                   1|              null|                   null|\n",
      "|     38|      8638|                   1|              null|                   null|\n",
      "|     40|     26706|                   0|              null|                   null|\n",
      "|     46|     40574|                   5|19.479166666666668|     21.188667617777828|\n",
      "|     49|       148|                   1|              null|                   null|\n",
      "|     50|     31528|                   3|              29.0|                   15.0|\n",
      "|     61|      4818|                   1|              null|                   null|\n",
      "|     66|     44022|                   0|              null|                   null|\n",
      "|     71|      6336|                   6|16.206666666666667|      7.777586029382411|\n",
      "|     74|     38758|                   1|              null|                   null|\n",
      "|     76|      2122|                   1|              null|                   null|\n",
      "|     77|     31035|                   1|              null|                   null|\n",
      "|     90|     19204|                  43|0.5592467635255642|     1.2153164910851233|\n",
      "|     90|     29285|                  14|1.5563645755953448|      2.335695799067583|\n",
      "+-------+----------+--------------------+------------------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "opf2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opf2.coalesce(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"opf2.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
