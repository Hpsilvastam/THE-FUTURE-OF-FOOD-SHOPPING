{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Capstone Project - Final Submission\n",
    "\n",
    "# THE FUTURE OF FOOD SHOPPING\n",
    "\n",
    "*Insights from the Instacart Online Grocery Shopping Dataset 2017*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Overview\n",
    "\n",
    "Even though all the other notebooks from this project were written in Pyspark or Python the final submission was written in R, due to computational resource issues in the parameter tuning of Xgboost.\n",
    "Here we will focus on the Feature Engineering, Tuning the Parameters and Model Evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Read 46.1% of 32434489 rows\r",
      "Read 76.0% of 32434489 rows\r",
      "Read 32434489 rows and 4 (of 4) columns from 0.538 GB file in 00:00:04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Joining, by = \"aisle_id\"\n",
      "Joining, by = \"department_id\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>used</th><th scope=col>(Mb)</th><th scope=col>gc trigger</th><th scope=col>(Mb)</th><th scope=col>max used</th><th scope=col>(Mb)</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>Ncells</th><td>   615044</td><td>  32.9   </td><td>  6336089</td><td> 338.4   </td><td> 20885653</td><td>1115.5   </td></tr>\n",
       "\t<tr><th scope=row>Vcells</th><td>436507209</td><td>3330.3   </td><td>683370284</td><td>5213.8   </td><td>679998362</td><td>5188.0   </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllll}\n",
       "  & used & (Mb) & gc trigger & (Mb) & max used & (Mb)\\\\\n",
       "\\hline\n",
       "\tNcells &    615044 &   32.9    &   6336089 &  338.4    &  20885653 & 1115.5   \\\\\n",
       "\tVcells & 436507209 & 3330.3    & 683370284 & 5213.8    & 679998362 & 5188.0   \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | used | (Mb) | gc trigger | (Mb) | max used | (Mb) | \n",
       "|---|---|\n",
       "| Ncells |    615044 |   32.9    |   6336089 |  338.4    |  20885653 | 1115.5    | \n",
       "| Vcells | 436507209 | 3330.3    | 683370284 | 5213.8    | 679998362 | 5188.0    | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "       used      (Mb)   gc trigger (Mb)   max used  (Mb)  \n",
       "Ncells    615044   32.9   6336089   338.4  20885653 1115.5\n",
       "Vcells 436507209 3330.3 683370284  5213.8 679998362 5188.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import the necessaries libraries for the code\n",
    "library(data.table)\n",
    "library(dplyr)\n",
    "library(tidyr)\n",
    "\n",
    "# Load Data\n",
    "path <- \"D:/DADOS USUARIO/Documents/springboard/capstone project/Instacart Kaggle\"\n",
    "\n",
    "aisles <- fread(file.path(path, \"aisles.csv\"))\n",
    "departments <- fread(file.path(path, \"departments.csv\"))\n",
    "orderp <- fread(file.path(path, \"order_products__prior.csv\"))\n",
    "ordert <- fread(file.path(path, \"order_products__train.csv\"))\n",
    "orders <- fread(file.path(path, \"orders.csv\"))\n",
    "products <- fread(file.path(path, \"products.csv\"))\n",
    "\n",
    "# Reshape data \n",
    "aisles$aisle <- as.factor(aisles$aisle)\n",
    "departments$department <- as.factor(departments$department)\n",
    "orders$eval_set <- as.factor(orders$eval_set)\n",
    "products$product_name <- as.factor(products$product_name)\n",
    "\n",
    "products <- products %>% \n",
    "  inner_join(aisles) %>% inner_join(departments) %>% \n",
    "  select(-aisle_id, -department_id)\n",
    "rm(aisles, departments)\n",
    "\n",
    "ordert$user_id <- orders$user_id[match(ordert$order_id, orders$order_id)]\n",
    "\n",
    "orders_products <- orders %>% inner_join(orderp, by = \"order_id\")\n",
    "\n",
    "rm(orderp)\n",
    "gc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Feature Engineering\n",
    "\n",
    "This step is fundamental to the application of machine learning, and is a reason of struggle for a lot data scientists as it is a mix of domain knowledge and art. A good definition is the process of transforming data to create model inputs.\n",
    "For this project we have created 3 types of features:\n",
    "\n",
    "1. **Products**: *What this product is like?*\n",
    "2. **Users**: *What this user is like?*\n",
    "3. **Products-Users**: *What is the user relation with the product?*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>used</th><th scope=col>(Mb)</th><th scope=col>gc trigger</th><th scope=col>(Mb)</th><th scope=col>max used</th><th scope=col>(Mb)</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>Ncells</th><td>   564772</td><td>  30.2   </td><td> 15453322</td><td> 825.3   </td><td> 20885653</td><td>1115.5   </td></tr>\n",
       "\t<tr><th scope=row>Vcells</th><td>436242851</td><td>3328.3   </td><td>984229208</td><td>7509.1   </td><td>715479663</td><td>5458.7   </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllll}\n",
       "  & used & (Mb) & gc trigger & (Mb) & max used & (Mb)\\\\\n",
       "\\hline\n",
       "\tNcells &    564772 &   30.2    &  15453322 &  825.3    &  20885653 & 1115.5   \\\\\n",
       "\tVcells & 436242851 & 3328.3    & 984229208 & 7509.1    & 715479663 & 5458.7   \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | used | (Mb) | gc trigger | (Mb) | max used | (Mb) | \n",
       "|---|---|\n",
       "| Ncells |    564772 |   30.2    |  15453322 |  825.3    |  20885653 | 1115.5    | \n",
       "| Vcells | 436242851 | 3328.3    | 984229208 | 7509.1    | 715479663 | 5458.7    | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "       used      (Mb)   gc trigger (Mb)   max used  (Mb)  \n",
       "Ncells    564772   30.2  15453322   825.3  20885653 1115.5\n",
       "Vcells 436242851 3328.3 984229208  7509.1 715479663 5458.7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Products \n",
    "prd <- orders_products %>%\n",
    "  arrange(user_id, order_number, product_id) %>%\n",
    "  group_by(user_id, product_id) %>%\n",
    "  mutate(product_time = row_number()) %>%\n",
    "  ungroup() %>%\n",
    "  group_by(product_id) %>%\n",
    "  summarise(\n",
    "    prod_orders = n(),\n",
    "    prod_reorders = sum(reordered),\n",
    "    prod_first_orders = sum(product_time == 1),\n",
    "    prod_second_orders = sum(product_time == 2)\n",
    "  )\n",
    "\n",
    "prd$prod_reorder_probability <- prd$prod_second_orders / prd$prod_first_orders\n",
    "prd$prod_reorder_times <- 1 + prd$prod_reorders / prd$prod_first_orders\n",
    "prd$prod_reorder_ratio <- prd$prod_reorders / prd$prod_orders\n",
    "\n",
    "prd <- prd %>% select(-prod_reorders, -prod_first_orders, -prod_second_orders)\n",
    "\n",
    "rm(products)\n",
    "gc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Products\n",
    "\n",
    "* **times_prod_ordered**: number the products ordered.\n",
    "* **prod_reorder_probability**: probability of the product being reordered. Ex. every reorder(bought a second time) of bananas divided by every first order of bananas.\n",
    "* **prod_reorder_times**: how many times the product was reordered\n",
    "* **prod_reorder_ratio**: ratio of a product reorders. Ex. times bananas were reordered divided by the times bananas were ordered.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Joining, by = \"user_id\"\n",
      "Joining, by = \"user_id\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>used</th><th scope=col>(Mb)</th><th scope=col>gc trigger</th><th scope=col>(Mb)</th><th scope=col>max used</th><th scope=col>(Mb)</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>Ncells</th><td>   564864</td><td>  30.2   </td><td> 12362657</td><td> 660.3   </td><td> 20885653</td><td>1115.5   </td></tr>\n",
       "\t<tr><th scope=row>Vcells</th><td>437995949</td><td>3341.7   </td><td>984229208</td><td>7509.1   </td><td>715479663</td><td>5458.7   </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllll}\n",
       "  & used & (Mb) & gc trigger & (Mb) & max used & (Mb)\\\\\n",
       "\\hline\n",
       "\tNcells &    564864 &   30.2    &  12362657 &  660.3    &  20885653 & 1115.5   \\\\\n",
       "\tVcells & 437995949 & 3341.7    & 984229208 & 7509.1    & 715479663 & 5458.7   \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | used | (Mb) | gc trigger | (Mb) | max used | (Mb) | \n",
       "|---|---|\n",
       "| Ncells |    564864 |   30.2    |  12362657 |  660.3    |  20885653 | 1115.5    | \n",
       "| Vcells | 437995949 | 3341.7    | 984229208 | 7509.1    | 715479663 | 5458.7    | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "       used      (Mb)   gc trigger (Mb)   max used  (Mb)  \n",
       "Ncells    564864   30.2  12362657   660.3  20885653 1115.5\n",
       "Vcells 437995949 3341.7 984229208  7509.1 715479663 5458.7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Users \n",
    "users <- orders %>%\n",
    "  filter(eval_set == \"prior\") %>%\n",
    "  group_by(user_id) %>%\n",
    "  summarise(\n",
    "    user_orders = max(order_number),\n",
    "    user_period = sum(days_since_prior_order, na.rm = T),\n",
    "    user_mean_days_since_prior = mean(days_since_prior_order, na.rm = T)\n",
    "  )\n",
    "\n",
    "us <- orders_products %>%\n",
    "  group_by(user_id) %>%\n",
    "  summarise(\n",
    "    user_total_products = n(),\n",
    "    user_reorder_ratio = sum(reordered == 1) / sum(order_number > 1),\n",
    "    user_distinct_products = n_distinct(product_id)\n",
    "  )\n",
    "\n",
    "users <- users %>% inner_join(us)\n",
    "users$user_average_basket <- users$user_total_products / users$user_orders\n",
    "\n",
    "us <- orders %>%\n",
    "  filter(eval_set != \"prior\") %>%\n",
    "  select(user_id, order_id, eval_set,\n",
    "         time_since_last_order = days_since_prior_order)\n",
    "\n",
    "users <- users %>% inner_join(us)\n",
    "\n",
    "rm(us)\n",
    "gc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Users\n",
    "\n",
    "* **user_mean_days_since_prior**: the average number of days that the user orders\n",
    "* **user_orders**: number of orders made by the user\n",
    "* **user_period**: number of days that a user made the first order\n",
    "* **user_total_prod**: number of products bought by the user\n",
    "* **user_reorder_ratio**: ratio of reorders made by the user.\n",
    "* **distinct**: number of distinct products ordered by the user\n",
    "* **user_average_basket**: the average number of products in a order\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>used</th><th scope=col>(Mb)</th><th scope=col>gc trigger</th><th scope=col>(Mb)</th><th scope=col>max used</th><th scope=col>(Mb)</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>Ncells</th><td>   564763 </td><td>  30.2    </td><td>  11449744</td><td> 611.5    </td><td> 20885653 </td><td>1115.5    </td></tr>\n",
       "\t<tr><th scope=row>Vcells</th><td>500011772 </td><td>3814.8    </td><td>1181155049</td><td>9011.5    </td><td>980362119 </td><td>7479.6    </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllll}\n",
       "  & used & (Mb) & gc trigger & (Mb) & max used & (Mb)\\\\\n",
       "\\hline\n",
       "\tNcells &    564763  &   30.2     &   11449744 &  611.5     &  20885653  & 1115.5    \\\\\n",
       "\tVcells & 500011772  & 3814.8     & 1181155049 & 9011.5     & 980362119  & 7479.6    \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | used | (Mb) | gc trigger | (Mb) | max used | (Mb) | \n",
       "|---|---|\n",
       "| Ncells |    564763  |   30.2     |   11449744 |  611.5     |  20885653  | 1115.5     | \n",
       "| Vcells | 500011772  | 3814.8     | 1181155049 | 9011.5     | 980362119  | 7479.6     | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "       used      (Mb)   gc trigger (Mb)   max used  (Mb)  \n",
       "Ncells    564763   30.2   11449744  611.5  20885653 1115.5\n",
       "Vcells 500011772 3814.8 1181155049 9011.5 980362119 7479.6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Products-Users\n",
    "data <- orders_products %>%\n",
    "  group_by(user_id, product_id) %>% \n",
    "  summarise(\n",
    "    up_orders = n(),\n",
    "    up_first_order = min(order_number),\n",
    "    up_last_order = max(order_number),\n",
    "    up_average_cart_position = mean(add_to_cart_order))\n",
    "\n",
    "rm(orders_products, orders)\n",
    "\n",
    "data <- data %>% \n",
    "  inner_join(prd, by = \"product_id\") %>%\n",
    "  inner_join(users, by = \"user_id\")\n",
    "\n",
    "data$up_order_rate <- data$up_orders / data$user_orders\n",
    "data$up_orders_since_last_order <- data$user_orders - data$up_last_order\n",
    "data$up_order_rate_since_first_order <- data$up_orders / (data$user_orders - data$up_first_order + 1)\n",
    "\n",
    "data <- data %>% \n",
    "  left_join(ordert %>% select(user_id, product_id, reordered), \n",
    "            by = c(\"user_id\", \"product_id\"))\n",
    "\n",
    "rm(ordert, prd, users)\n",
    "gc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Products-Users\n",
    "\n",
    "* **user_mean_days_since_prior**: the average number of days that the user orders\n",
    "* **user_orders**: number of orders made by the user\n",
    "* **user_period**: number of days that a user made the first order\n",
    "* **user_total_prod**: number of products bought by the user\n",
    "* **user_reorder_ratio**: ratio of reorders made by the user.\n",
    "* **distinct**: number of distinct products ordered by the user\n",
    "* **user_average_basket**: the average number of products in a order\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Tuning the Parameters\n",
    "\n",
    "XGBoost is a powerful tool, the difficult part is in improving the model, because this algorithm uses multiple parameters. To improve the model, parameter tuning is necessary. \n",
    "\n",
    "For tuning the parameters I followed AARSHAY JAINÂ´s great tutorial steps [https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Step 1:**\n",
    "\n",
    "Fix learning rate and number of estimators for tuning tree-based parameters\n",
    "\n",
    "max_depth = 6 : This should be between 3-10\n",
    "\n",
    "min_child_weight = 1 : A smaller value is chosen \n",
    "\n",
    "gamma = 0 : A smaller value can be chosen for starting\n",
    "\n",
    "subsample, colsample_bytree = 1 : Typical values range between 0.5-0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>used</th><th scope=col>(Mb)</th><th scope=col>gc trigger</th><th scope=col>(Mb)</th><th scope=col>max used</th><th scope=col>(Mb)</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>Ncells</th><td>   565539 </td><td>  30.3    </td><td>  9159795 </td><td> 489.2    </td><td>  20885653</td><td>1115.5    </td></tr>\n",
       "\t<tr><th scope=row>Vcells</th><td>240509883 </td><td>1835.0    </td><td>944924039 </td><td>7209.2    </td><td>1179195249</td><td>8996.6    </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllll}\n",
       "  & used & (Mb) & gc trigger & (Mb) & max used & (Mb)\\\\\n",
       "\\hline\n",
       "\tNcells &    565539  &   30.3     &   9159795  &  489.2     &   20885653 & 1115.5    \\\\\n",
       "\tVcells & 240509883  & 1835.0     & 944924039  & 7209.2     & 1179195249 & 8996.6    \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | used | (Mb) | gc trigger | (Mb) | max used | (Mb) | \n",
       "|---|---|\n",
       "| Ncells |    565539  |   30.3     |   9159795  |  489.2     |   20885653 | 1115.5     | \n",
       "| Vcells | 240509883  | 1835.0     | 944924039  | 7209.2     | 1179195249 | 8996.6     | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "       used      (Mb)   gc trigger (Mb)   max used   (Mb)  \n",
       "Ncells    565539   30.3   9159795   489.2   20885653 1115.5\n",
       "Vcells 240509883 1835.0 944924039  7209.2 1179195249 8996.6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'caret' was built under R version 3.4.3\"Loading required package: lattice\n",
      "Loading required package: ggplot2\n",
      "Warning message:\n",
      "\"package 'ggplot2' was built under R version 3.4.3\"Warning message:\n",
      "\"package 'xgboost' was built under R version 3.4.3\"\n",
      "Attaching package: 'xgboost'\n",
      "\n",
      "The following object is masked from 'package:dplyr':\n",
      "\n",
      "    slice\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:03:26] Tree method is automatically selected to be 'approx' for faster speed. to use old behavior(exact greedy algorithm on single machine), set tree_method to 'exact'\n",
      "[13:03:30] Tree method is automatically selected to be 'approx' for faster speed. to use old behavior(exact greedy algorithm on single machine), set tree_method to 'exact'\n",
      "[13:03:35] Tree method is automatically selected to be 'approx' for faster speed. to use old behavior(exact greedy algorithm on single machine), set tree_method to 'exact'\n",
      "[13:03:39] Tree method is automatically selected to be 'approx' for faster speed. to use old behavior(exact greedy algorithm on single machine), set tree_method to 'exact'\n",
      "[13:03:43] Tree method is automatically selected to be 'approx' for faster speed. to use old behavior(exact greedy algorithm on single machine), set tree_method to 'exact'\n",
      "[1]\ttrain-error:0.092236+0.000059\ttest-error:0.092279+0.000487 \n",
      "[11]\ttrain-error:0.090572+0.000094\ttest-error:0.090637+0.000428 \n",
      "[21]\ttrain-error:0.090154+0.000098\ttest-error:0.090221+0.000409 \n",
      "[31]\ttrain-error:0.089890+0.000109\ttest-error:0.090029+0.000415 \n",
      "[41]\ttrain-error:0.089744+0.000127\ttest-error:0.089949+0.000391 \n",
      "[51]\ttrain-error:0.089623+0.000121\ttest-error:0.089895+0.000416 \n",
      "[61]\ttrain-error:0.089517+0.000127\ttest-error:0.089855+0.000391 \n",
      "[71]\ttrain-error:0.089438+0.000112\ttest-error:0.089820+0.000388 \n",
      "[81]\ttrain-error:0.089353+0.000123\ttest-error:0.089779+0.000408 \n",
      "[91]\ttrain-error:0.089271+0.000112\ttest-error:0.089760+0.000418 \n",
      "[100]\ttrain-error:0.089197+0.000102\ttest-error:0.089748+0.000409 \n",
      "[13:16:56] Tree method is automatically selected to be 'approx' for faster speed. to use old behavior(exact greedy algorithm on single machine), set tree_method to 'exact'\n",
      "[1]\tval-error:0.092368\ttrain-error:0.092260 \n",
      "[11]\tval-error:0.090797\ttrain-error:0.090616 \n",
      "[21]\tval-error:0.090282\ttrain-error:0.090104 \n",
      "[31]\tval-error:0.090104\ttrain-error:0.089904 \n",
      "[41]\tval-error:0.090055\ttrain-error:0.089787 \n",
      "[51]\tval-error:0.089979\ttrain-error:0.089672 \n",
      "[61]\tval-error:0.089924\ttrain-error:0.089573 \n",
      "[71]\tval-error:0.089895\ttrain-error:0.089496 \n",
      "[81]\tval-error:0.089848\ttrain-error:0.089387 \n",
      "[91]\tval-error:0.089803\ttrain-error:0.089312 \n",
      "[100]\tval-error:0.089791\ttrain-error:0.089247 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Confusion Matrix and Statistics\n",
       "\n",
       "          Reference\n",
       "Prediction       0       1\n",
       "         0 2075431  115951\n",
       "         1  218014  133002\n",
       "                                          \n",
       "               Accuracy : 0.8686          \n",
       "                 95% CI : (0.8682, 0.8691)\n",
       "    No Information Rate : 0.9021          \n",
       "    P-Value [Acc > NIR] : 1               \n",
       "                                          \n",
       "                  Kappa : 0.3713          \n",
       " Mcnemar's Test P-Value : <2e-16          \n",
       "                                          \n",
       "            Sensitivity : 0.9049          \n",
       "            Specificity : 0.5342          \n",
       "         Pos Pred Value : 0.9471          \n",
       "         Neg Pred Value : 0.3789          \n",
       "             Prevalence : 0.9021          \n",
       "         Detection Rate : 0.8163          \n",
       "   Detection Prevalence : 0.8619          \n",
       "      Balanced Accuracy : 0.7196          \n",
       "                                          \n",
       "       'Positive' Class : 0               \n",
       "                                          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"Setting row names on a tibble is deprecated.\""
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAANlBMVEUAAAAXFxcqKio8PDxN\nTU1dXV1tbW18fHyMjIybm5uqqqq4uLi+vr7GxsbT09PV1dXi4uL///9MECurAAAACXBIWXMA\nABJ0AAASdAHeZh94AAAgAElEQVR4nO2dDXubTJJF2/nw5H1nszH//89ObAlooEEIbtHVqnP2\n2YwjpOtrwRlVSLY2dQBwmlS7AMArgEgAAhAJQAAiAQhAJAABiAQgAJEABCASgABEAhCASAAC\nEAlAACIBCEAkAAGIBCAAkQAEIBKAAEQCEIBIAAIQCUAAIgEIQCQAAYgEIACRAAQgEoAARAIQ\ngEgAAhAJQAAiAQhAJAABiAQgAJEABCASgABEAhCASAACEAlAACIBCEAkAAGIBCAAkQAEIBKA\nAEQCEIBIAAIQCUAAIgEIQCQAAYgEIACRAAQgEoAARAIQgEgAAhAJQAAiAQhAJAABiAQgAJEA\nBCASgABEAhCASAACEAlAACIBCEAkAAGIBCAAkQAEIBKAAEQCEIBIAAIQCUAAIgEIQCQAAYgE\nIACRAAQgEoAARAIQgEgAAhAJQAAiAQhAJAABiAQgAJEABCASgABEAhCASAACEAlAACIBCEAk\nAAGIBCAAkQAEIBKAAEQCEIBIAAIQCUAAIgEIQCQAAYgEIACRAAQgEoAARAIQgEgAAhAJQEBM\nkf6/pVjKtlAWkdzHUraFsjFFAhCDSAACYooUdgAxjw1bFpHcx1K2hbIxRQIQg0gAAmKKFHYA\nMY8NWxaR3MdStoWyMUUCEINIAAJiihR2ADGPDVsWkdzHUraFsjFFAhCDSAACYooUdgAxjw1b\nFpHcx1K2hbIxRQIQg0gAAmKKFHYAMY8NWzamSP8HsMGBSwqRAOYcuKQQCWDOgUsKkQDmHLik\nEAlgzoFLCpEA5hy4pBAJYM6BSwqRAOYcuKQQCWDOgUuqRZG2Ou/7eWqfKPCN+KL0CiKBLeKL\nshbp63///s9YLn1+/fnY54Mp//39FffHbo+nRz9V7RMFvjl40XrjLlLqMlGyR24qTUTrn3sT\nqnv4Y9U+UeCbgxetNwYl5iKNX6WUUtb966v+sdvBTWqfKPDNwYvWGztEGh7Nj2efSA+ofaLA\nNwcvWm/cx7flaDce60adxuP9n586Rjs4xbGL1h9fQpRuNnTd9GbDcHh8FTcb4DRHrtkDr7kG\ny2a1TxT45sAl5V2k9ODWwaPjZWqfKPDN4cs1GrVPFPjmwCWFSABzDlxSiAQw58AlhUgAcw5c\nUogEMOfAJRVTpLBrDM1jw5aNKRKAGEQCEBBTpLADiHls2LKI5D6Wsi2UjSlS7btCcBWXXVKI\nBK/MZZcUIsErs3kZMNqdpfbphavYvAwQ6Sy1Ty9cxWWXFCLBK3PZJYVI8MpsXgaMdmepfXrh\nKjYvA0Q6S+3TC1dx2SV1tUjb32/9aNr43c7wjNqnF65i7xVxGl8i7X0dIsE+Ni8Dz6PdUwvw\nU+FZt8fyX/vc8bH88e3t+ivUPr1wFZuXQQMi7VqAX1pKnO9UnR6fH3kQvknt0wtXobimd2H1\nibSxt3tcgJ9f8GkpSr6y7vZhk4bnPQ7fovbphas4cy0/RQWR8u+bpi9Npefcvx4y05ZIixcW\nqX164So2LwPno93uBfhPj3bL9fnr4ZvUPr1wFZuXgWeRnlqAX7rZ0M1uLIw981fPbzYUwreo\nfXrhKs5f0DuRi2SYqqP26YWruOySshRp34L7g2vyj23Pv1P79MJVbF4Grke7Jqh9euEqNi8D\nRDpL7dMLV3HZJYVI8MpcdkkhErwym5cBo91Zap9euIrNywCRAJyBSAACYooUdrGueWzYsojk\nPpayLZSNKRKAmJgi1b6XdDm13/DXB5FCUHgP/E9L5rGMdmepfV1fTuE98H9tmsci0llqX9eX\nU/sNf30QKQS13/DXB5FCUHgP/E9L5rGMdmepfV1fTuE98H9tmsci0llqX9eXU/sNf30QKQS1\n3/DXx1Ckrejise0XzFbgPRswofZ1fTmF98D/tGQe28ho97RI+5+PSE9SeA/8X5vmsU5EGnY6\nlpbTFVfaD4/dl9MtX7i2Qr/flpe9fNd3XKH2dX05x88y7OO8SMulqmm29zS76rPdqM/sWe1S\n/q1Yov80x88y7EPwibT0YbzApyvt+/Wq/WfHVLGJQvPjw+Ms0T9C4T3wPy2Zxzob7TZEmn2L\nyZHlH3vWV+hnInWT1z76jivUvq4vp/Ae+L82zWP9iLR7X35+vP/TTLd84YPRruvmkY++4wq1\nr+vLOXySYScnRHpqX/74zVL/midW6E9Gu9SxRP9Zjp5i2MsZkTQBNah9XV9O4T3wPy2ZxzoZ\n7fKAg5vwDV+4Se3r+nIK74H/a9M81pVITVL7ur6c2m/464NIIaj9hr8+iBSCwnvgf1oyj2W0\nO0vt6/pyCu+B/2vTPBaRzlL7ur6c2m/46xNTJAAxMUUKO4CYx4Yti0juYynbQtmYIgGIQSQA\nATFFqn0TTcHZ98D/tGQey2h3ltoSKDj7Hvi/Ns1jEekstSVQUPs9hAmI1Cq130OYgEitcvY9\n8D8tmccy2p2ltgQKzr4H/q9N81hEOkttCRTUfg9hAiK1Su33ECYgUqucfQ/8T0vmsYx2C578\nMWpLoODsW+b/2jSPRaQFiAR1aUSkYYNkaRXefH3+4+1CtSVQYPp+w7O0JdJyq2ua7VidrXBd\nobYECs6+pf6nJfPYgKPdcovxYuv3uD7/8bq72hIoOPuW+r82zWMRafHIbH0+ox1cTDsi7V7Y\nz2gH19OISE8t7Odmwx78T0vmsQFHuxuysrUlUHD2PfB/bZrHBhfp1Pr8G7UlUKB4Q0FGUyLJ\nqC2BgtrvIUxApFY5+x74n5bMY8OOdjJqS6Dg7Hvg/9o0j0Wks9SWQEHt9xAmIFKr1H4PYUJM\nkcIOIOaxYcsikvtYyrZQNqZIAGIQCUBATJGM/ojvfwAxjw1bFpEQyX9qA2URSSgSxAWREAkE\nIJJQJP8DiHls2LKIhEj+Uxsoi0hCkSAuiIRIIACRhCL5H0DMY8OWRSRE8p/aQNk6Ij36ro9b\npcXX9zVDu76/kUgQl9cRaddCuzuIBGLsRer3Og7b7VPKLv3s0bWd+MWU+4q77Ot031tsuyBy\nM9b/AGIeG7bsBSJll/qoSn5sXJFa2om/kpLmrxpGO0SqGBu27BWfSMsl3ZNhLPsYKe7E30qZ\nvDZ1ec4WRiJBXByItPqs7qFI018RCapx2WjXFUe7riTGdCf+Skr+J6rpaLfjhzISyf8AYh4b\ntux1Nxu64s2G7NHxWV3Kf19MGW42dKmbqLTr1h0iUVaces1od3FKLZEgLrVE2r8JPz29NP/x\ncxEJxNiL5BEjkfwPIOaxYcsiEiL5T22gLCIJRYK4IBIigQBEEorkfwAxjw1bNqZIYU+3eWzY\nsjFFAhCDSAACYooUdgAxjw1bNqZIRrfq/J9u89iwZRFJKBLEBZEQCQQgklAk/wOIeWzYsoiE\nSP5TGyiLSEKRIC6IhEggAJGEIvkfQMxjw5ZFJETyn9pAWUQSigRxuViktP0d14+ljd/tePkM\nRAIx14ukeKFTkfwPIOaxYcteK1Ia9tCNu+yyQ/Mj2fGUP5Y/vr17fwVEoqw4tcZod7/kC8uL\nS6uI++PzI/nL0mxBq3hlsfIdgNekikiDUJOPlm4uSn789mGTutkq1alxy937KyASiKkqUvnY\nKNLk4CBQ2hJp8cIiRiL5H0DMY8OWrS3SU6Pdcrn+5ImF3fsrIBJlxalXi5QykYo3G7rZjYWx\nZ343YX6zoZseflTDSCSIC38hi0ggoL5Ij3bkrxx/erV+jpFI/gcQ89iwZeuLVANEoqw4FZGE\nIkFcEAmRQAAiCUXyP4CYx4YtG1OksKfbPDZs2ZgiAYhBJAABMUUKO4CYx4Yti0juYynbQtmY\nInHfG8QgEiKBAEQSiuR/ADGPDVsWkRDJf2oDZRGJ0Q4EIBIigQBEYrTzn9pAWURCJP+pDZRF\nJEY7EIBIiAQCfIn0qE1hdcOhH4HRjrLi1BcQ6QCIRFlxaj2R+pXd2Ra7+TrIYbfd6pb84lb+\nx9uFGO1ATEWRUrb/flAlP5btXO0XSma/vz/z/tvZvuMHPxYigZian0jLtd2TBcbDPvytLfnT\nHchptod1DUY7yopTHYu0+qxuVaQ8ewtEoqw4tfpo1xVHu26q0LARfKLT/ZnzrfyMdnA99W82\ndMWbDdmj47Oymw1DSmErPzcb4HLqjna1YLSjrDjVm0j7d+Of2aKPSJQVp/r6C9mrYLQDMYiE\nSCAAkRjt/Kc2UBaREMl/agNlY4oEIAaRAATEFCnsAGIeG7YsIrmPpWwLZWOKBCAmpkj8NRKI\nQSShSP4HEPPYsGURCZH8pzZQFpEY7UAAIiESCEAkRjv/qQ2URSRE8p/aQFlEYrQDAYiESCAA\nkRjt/Kc2ULY9kbYap50/ECJRVpz6ciLtgtEOxNQVaVj8mG2wG3fbFY6UFumPz0zZ4U0QCcR4\nEGny67h2OHWLI5Ntq+VnKjetPvvj+B9AzGPDlvUgUtd/7MzX4+9fpH97KiJVjw1b1oVI2V2C\ntDy2FGnlmdVFgrhUF2n2ibOix+hHeZF+9ucnRIIKVL5rl98dmK/H77rlkfIi/eFI5ZsN/gcQ\n89iwZdu7/a0AkSgrTvUokv0ifUY7EONRJHsQCcQgEqOd/9QGyiISIvlPbaAsIjHagYCYIgGI\niSlS2AHEPDZsWURyH0vZFsrGFAlADCIBCIgpktH9Of8DiHls2LKIhEj+Uxsoi0j8jREIQCRE\nAgGIxGjnP7WBsoiESP5TGyiLSIx2IACREAkEIBKjnf/UBsoiEiL5T22gbHMilQs/+WMw2oEY\nREIkEOBapH7hd7b6biy8vln/wF47TV//A4h5bNiyvkVK2fb8QZr7sW5ts/6RTauavv5Pt3ls\n2LK+ReqWS7/T2rFxs/7jTXeMdiDmdUQaHu0e/1SIBGJ8i5QWssxHu660iZ/Rrlps2LK+Reqy\n7fkrNxuGR9PsuVsgEmXFqc5FMoLRDsQ0KNLRzfkZiARiXItkBqMdZcWpiIRI/lMbKItIjHYg\nAJEQCQTEFCnsAGIeG7YsIrmPpWwLZWOKBCAGkQAExBQp7ABiHhu2bEyRjO7Y+T/d5rFhyyIS\nt75BACIhEghAJEY7/6kNlEUkRPKf2kBZRGK0AwGIhEggAJEY7fynNlAWkRDJf2oDZRGJ0Q4E\nuBHpUZE9RXf/MIgEYpoRSQqjHWXFqbLrd1jVeNs/N6x2vB1L2eP3o/kmoNlmutki/D5tkTGm\n375bWnznFRCJsuJUsUiTX7Ndjl1aHM2++XJXamk58erxlMs0/c4rMNqBGP0nUtflC+37Y8Nn\nSv9htFTl/pFzP5QVS3OF+u+Qv3r5nA0QCcSoReqtmUQPImXPLH3mLJ42CZiItPjOHkTyP4CY\nx4YtqxSpMJ6NxyaPLz+R+j8ldaOST452xe+8AiJRVpyqu1mW3TDIFtr33yRNbkXMVBg1Gj/X\nCjcbum72Hfr0+U0KRju4Gp1IMp6tdOBHQCQQYyzSo4X3heNp82jh1X5E8j+AmMeGLevwE+kC\nEImy4lREYrQDAYiESCAAkRjt/Kc2UDamSGFPt3ls2LIxRQIQg0gAAmKKFHYAMY8NWxaR3MdS\ntoWyMUXi9jeIQSREAgGIxN8j+U9toCwiIZL/1AbKIhKjHQhAJEQCAYjEaOc/tYGyiIRI/lMb\nKItIjHYgoBWRVv/P0Xc8vASRQAwiMdr5T22grE+RhlV22c6tbA9XvotrWNU12R/+4MdCJMqK\nU12LlK2oW9kAOXniuKDy0U/FaAdiXIvUzZaxpnwZa+GJ/XZxFkTC1bQj0riEdUuk4dFNGO0o\nK051K1IqjHZpbbQbXzI9vAYiUVac6lOkfj347GbDeDdhfrOhmx5+FM9oB2KcivSFXTdEAjH+\nRVrZ73107fcXjHaUFad6FskORKKsOBWRGO1AACIhEghAJEY7/6kNlEUkRPKf2kDZmCIBiEEk\nAAExRQo7gJjHhi2LSO5jKdtC2ZgiAYiJKRJ/hQRiEEmI/wHEPDZsWUQS4v90m8eGLYtIAAIQ\nCUAAIgnxP4CYx4Yti0hC/J9u89iwZREJQAAiAQh4IZHSytcFGO0oK059IZFyEKlSbNiyzkUa\nVj/2+/Oz9Y/3tXfDo9nzKokEcWlCpMmv2W7I6aPZ8x7t6UIkENOESMNHz+SzZr42f/rrJox2\nlBWntiFStjl/IdLsa0SqGhu2rH+RCp8392NpLg+jHdTCuUj5avz5gvzpzYbJTQluNsDFeBdp\ngxPVGe0oK05tUqR0aoN+h0h2sWHLNinSaRjtQAwiAQhAJCH+BxDz2LBlEUmI/9NtHhu2LCIB\nCIgpEoCYmCKFHUDMY8OWRST3sZRtoWxMkQDEIBKAgJgiGd2y8z+AmMeGLYtIQvyfbvPYsGUR\nCUAAIgEIQCQh/gcQ89iwZRFJiP/TbR4btiwiAQhAJAABiCTE/wBiHhu2bLMiLYunrYNTEImy\n4lREAhBwsUhbS/GXG+oWi/Gz1UH5VrvhlfenPNwwhEggpopIK0vxi8fy7an5MuLSntX86Zsw\n2lFWnFrrE6lbLsXv0uRjqD+WL8nPttmNn1xjXim0CCJRVpxaR6SNpfjjB8v47OXe7+njxRdu\nwWgHYq4XaX0pfv+h0s00SZujXfIz2kFcrr5rt7EUf6LKeCzXqHSzIbs1MX5Z52aD/wHEPDZs\n2YZufwurIhJlxakuRHq8Ez8/fnaDfsdoB3JciHQ5iARiEEmI/wHEPDZsWUQS4v90m8eGLYtI\nAAIQCUBATJHCDiDmsWHLIpL7WMq2UDamSABiEAlAQEyRwg4g5rFhy8YUib9Hoqw4FZEABCAS\ngABEEuJ/ADGPDVsWkYT4P93msWHLIhKAAEQCEIBIQvwPIOaxYcsikhD/p9s8NmzZhkTaqJqt\ntNsFox2IeQ2Rnv05EAnEGIo0rG3MdkA+syZ/tmo/31a82J+f8v12j3cMMdpRVpxqL1I2dM03\nrD5Yk1943u1AWsR0i4c2QSTKilMv+ESaKPDkmvzp0VJwGj2sLhLE5VKR+mt/vDWQZs9Ls9eX\ndZw+lBAJqmMrUlqMds+sye+fsfhEYrRzGxu2rOVdu3S7wie3D7IPkkdr8vObEPcv+s7L/2d9\n02X6iFQrNmxZS5Ge/gY7n3u+M6MdiLlIJNGa/CRYoP8JIoEYc5FcwmhHWXEqIgnxf7rNY8OW\nRSQAAYgEIACRhPgfQMxjw5aNKVLY020eG7ZsTJEAxCASgICYIoUdQMxjw5ZFJPexlG2hbEyR\nuPsNYhAJQAAiCfE/gJjHhi2LSEL8n27z2LBlEQlAACIBCEAkIf4HEPPYsGURSYj/020eG7Ys\nIgEIaEWk0sq7wpN2gkggphGR0uZvHzy8hNGOsuJUjyINSx/ztULDGsivpZOTI918L1569GMh\nEmXFqY5Fyla0dil/JE0eHw9kG8QffAdGOxDjWKRuIdLXVxtrwPsl/A9/KEQCMc2JdP+6+NT+\nKbVE8j+AmMeGLetUpFQc7bq10W580fTwGohEWXGqR5FK2/ez0S513eJmQ/+i7PdbMNqBGJci\nfWHZDJFAjHeRHuzTP5jNaEdZcapfkSxBJMqKUxEJQAAiAQhAJCH+BxDz2LBlEUmI/9NtHhu2\nbEyRAMQgEoCAmCKFHUDMY8OWRST3sZRtoWxMkbj9DWIQCUAAIgnxP4CYx4Yti0hC/J9u89iw\nZREJQAAiAQhAJCH+BxDz2LBlEUmI/9NtHhu2LCIBCEAkAAF+RBq3aC32AG2WfOKpA4x2lBWn\nehJp+dXaA+sHEalubNiy14k0bHH8+mK26z7NVuTflg938535/drILCffD5n6rXfs/oaLuVqk\nya+TRaopP5oWT789b7lMPy2eOtl1XAaRQEyFT6SuW+y6z7Xo5iI9WPW9fOrjdXeMdpQVp14u\nUnYvoSBSf/SUSI9/KkSirDj1UpGWY9js2M7RrluMdonRDqpy4V27bMv9Ytd9frNhcQehW9xs\n6Lr8ZsNkfz43G6AGfm5/70HVltGOsuLU2iLt3YWfTi3Nn4NIlBWn1hapDox2IAaRAAQgkhD/\nA4h5bNiyiCTE/+k2jw1bFpEABMQUCUBMTJHCDiDmsWHLIpL7WMq2UDamSABiYorEvQYQg0hC\n/A8g5rFhyyKSEP+n2zw2bFlEAhCASAACEEmI/wHEPDZsWUQS4v90m8eGLYtIAAIQCUAAIgnx\nP4CYx4Yt61GktP7F1rMfPnMEkSgrTkUkAAG1RMrX1s1W6g+L8MeN+PO9dtkrpyvw9i/Rt/rB\nICZ1RZr8mn/+zNamDiKlfEnkqSX6+h+phQHEPDZs2eqfSF1XWqmfuZEf27f7e9cSfeHPMuD/\ndJvHhi1bWaTBmm5FpNknUrdHpMc/FaMdiKko0uZK/W4UKS1Gu24x2j2/RN/gZ4LAVLtrt7FS\nf7zH0Csyv9nQdd3ZJfoWP5P/AcQ8NmxZj7e/11G1RSTKilN9ibS+KV++RF+UBPCFL5GuApFA\nDCIJ8T+AmMeGLYtIQvyfbvPYsGURCUBATJEAxMQUKewAYh4btiwiuY+lbAtlY4oEIAaRAATE\nFInb35QVpyKSEP+n2zw2bFlEAhCASAACEEmI/wHEPDZsWUQS4v90m8eGLYtIAAIQCUAAIgnx\nP4CYx4Yti0hC/J9u89iwZV9EpCd/DEY7EINIAAJci5Rtxx+W340bwrN9+uNau/H5WzDaUVac\n2oBIk1/T7NhkGWu+X38TRKKsOLUBkVY37U+3F4/Hqy3Rh7i0INLapv3FPv3hOEv04WK8i7S1\naX8UafYsRrtqsWHLuhZpa9N+9vvJzYb+yCaIRFlxqm+RrGC0AzENiiRYp49IIKZBkQQw2lFW\nnIpIQvyfbvPYsGURCUAAIgEIiClS2AHEPDZsWURyH0vZFsrGFAlADCIBCIgpUtgBxDw2bFlE\nch9L2RbKxhQJQAwiAQiIKVLYAcQ8NmxZRHIfS9kWysYUCUAMIgEIiClS2AHEPDZsWURyH0vZ\nFsrGFAlAjD+RHjXaOp48/kAQAX/X3UmRdhF2ADGPDVu2rkj9su5sed3QaLnSrrAvf/rMlHYu\n0Q97us1jw5atLFKabr7vF32Px6b78+f78gvPHLetAlxH7U+k5QLvlB8bNuOv7Msfntn1jyES\n1MC5SKvPWu7Tf+YTKewAYh4btqyL0a4rjnbdVKHSvvzJM5/4RAp7us1jw5at/Yk023w/udmQ\nPbq2Lz9/5i1t180GADG1RQJ4CTyKtH9L/tF9+mEHEPPYsGVjfiaEPd3msWHLxhQJQAwiAQiI\nKVLYAcQ8NmxZRHIfS9kWysYUCUAMIgEIiClS2AHEPDZsWURyH0vZFsrGFAlADCIBCIgpUtgB\nxDw2bFlEch9L2RbKxhQJQAwiAQiIKVLYAcQ8NmxZRHIfS9kWysYUCUAMIgEIiClS2AHEPDZs\nWURyH0vZFsrGFAlADCIBCIgpUtgBxDw2bFlEch9L2RbKxhQJQAwiAQiIKVLYAcQ8NmxZRHIf\nS9kWysYUCUAMIgEIiClS2AHEPDZsWURyH0vZFsrGFAlADCIBCIgpUtgBxDw2bFlEch9L2RbK\nxhQJQAwiAQiIKVLYAcQ8NmxZRHIfS9kWysYUCUAMIgEIiClS2AHEPDZsWURyH0vZFsrGFAlA\nDCIBCIgpUtgBxDw2bFlEch9L2RbKxhQJQAwiAQiIKVLYAcQ8NmxZRHIfS9kWysYUCUAMIgEI\niClS2AHEPDZs2ZgiJYC/KC8pYVY7GP3UNrGUbaEsIrmPpWwLZRHJfSxlWyiLSO5jKdtCWURy\nH0vZFsoikvtYyrZQFpHcx1K2hbKI5D6Wsi2URST3sZRtoSwiuY+lbAtlEcl9LGVbKItI7mMp\n20LZmCIBiEEkAAGIBCAAkQAEIBKAAEQCEIBIAAIQCUAAIgEIQCQAAYgEIACRAAQgEoAARAIQ\ngEgAAhAJQEAIkd7f0tv7R/mBxTFJ7Ikl7YVCv9L6MUGssuyvbybv7BgrLPvxM6Wfv1e+47NE\nEOn711v/rfjA4pgk9vfx010o9LsPUpYdY5Vl378eePsQlx1jlWXfvh74XTz2NAFE+m96+939\nfkv/LTywOKaJ/Z1+qMp2n79La8cUscKyv9PPj89Pup/aslmssOz7Z977V96Jsj0BRHpP//79\n9Z/0n8IDi2Oa2F9HAsupf7O+3694ZdksVlj2xy3xM1hZNosVln1LH/fQM2V7Aoj0I/3pJv9V\nlj2wOKaJ/ZV+qcp26b27X/HKslmssuw9O4nLjrHysult4zs+QQCRUsr/Y/rA4pgm9kf69+ff\nP7wqyna/5w9KymaxyrJffKTv4rJjrLrs+5eYJ8oO8cdf2gpVRPriuyB1+aDs2hxEkpb9/NT4\n10Kkr1ht2X9Set/4jk/FH39pK1QQKaV//v436PuBMaSCSOKy3Z+3H6vHBLHKsr9+vH39uQiR\n9lBBpBsfB+6nVhDphqzsx9v31WPnY++/UZXtup+fTiLSHt7m71L2wOKYJvbOgdhiofvvlGUf\n/e546vdv68fOx95Rlf108u1U2aHR8Ze2wu2WzJ/57bU/4127P8fvLZVj7xw4McVCwx9mdGWX\n9TRl/3z7/mf9O56PlZYdok6UHWKOv7QV/vP1lwT/pvfCA4tjmtjbX1EcOTHFQvfLRlk2i1WW\n/Xe4DSAtO8YKy/ZR306V7QkgUoV/2fD+eUo+bn/NdzL1k/sVr/2XDUOssOyf8XaasmwWKyz7\n9S8bPn58/hmJf9mwi2/jHdPbtZM98O3o3dTN2I/bP+M68l9wi9TsC2XZ8Qth2Z9p/KdwwrJZ\nrPKdfZNcBnciiPTx9U97v768vYPZA9mX6thvh/4OfpGafaEsO4vVlE2ZSMKy81jVOztGnSh7\nJ4JIAOYgEoAARAIQgEgAAhAJQAAiAQhAJAABiAQgAJEABCASgABEAhCASAACEAlAACIBCEAk\nAAGIBCAAkQAEIBKAAEQCEIBIAAIQCUAAIgEIQCQAAYgEIACRAAQgEoAARAIQgEgAAhAJQAAi\nAQhAJBzll+cAAAC2SURBVAABiAQgAJEABCASgABEAhCASAACEAlAACIBCEAkAAGIBCAAkQAE\nIBKAAEQCEIBIAAIQCUAAIgEIQCQAAYgEIACRAAQgEoAARAIQgEgAAhAJQAAiAQhAJAABiAQg\nAJEABCASgABEAhCASAACEAlAACIBCEAkAAGIBCAAkQAEIBKAAEQCEIBIAAIQCUAAIgEIQCQA\nAYgEIACRAAQgEoAARAIQgEgAAhAJQAAiAQhAJAAB/wPosThqzg/mUgAAAABJRU5ErkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train / Test datasets \n",
    "train <- as.data.frame(data[data$eval_set == \"train\",])\n",
    "train$eval_set <- NULL\n",
    "train$user_id <- NULL\n",
    "train$product_id <- NULL\n",
    "train$order_id <- NULL\n",
    "train$reordered[is.na(train$reordered)] <- 0\n",
    "\n",
    "test <- as.data.frame(data[data$eval_set == \"test\",])\n",
    "test$eval_set <- NULL\n",
    "test$user_id <- NULL\n",
    "test$reordered <- NULL\n",
    "\n",
    "rm(data)\n",
    "gc()\n",
    "#splitting data using Caret package\n",
    "library(caret)\n",
    "trainIndex <- createDataPartition(train$reordered, p=0.7, list=FALSE, times=1)\n",
    "xgb_train <- train[trainIndex,]\n",
    "xgb_test <- train[-trainIndex,]\n",
    "\n",
    "# Model using default parameters\n",
    "library(xgboost)\n",
    "\n",
    "labels <- xgb_train$reordered\n",
    "ts_label <- xgb_test$reordered\n",
    "\n",
    "#drop a column\n",
    "mtx_train <- model.matrix(~.+0,data = xgb_train[,-20]) \n",
    "mtx_test <- model.matrix(~.+0,data = xgb_test[,-20])\n",
    "\n",
    "\n",
    "#preparing matrix \n",
    "dtrain <- xgb.DMatrix(data = mtx_train,label = labels) \n",
    "dtest <- xgb.DMatrix(data = mtx_test,label=ts_label)\n",
    "\n",
    "params <- list(\n",
    "  booster = \"gbtree\", \n",
    "  objective = \"binary:logistic\",\n",
    "  eta=0.3, \n",
    "  gamma=0, \n",
    "  max_depth=6, \n",
    "  min_child_weight=1, \n",
    "  subsample=1, \n",
    "  colsample_bytree=1\n",
    "  )\n",
    "\n",
    "xgbcv <- xgb.cv( params = params, data = dtrain, nrounds = 100, nfold = 5, showsd = T, stratified = T, print_every_n = 10, early_stop_rounds = 20, maximize = F)\n",
    "#after 100 iterations no conversion last test-error:0.089773\n",
    "\n",
    "#first default - model training\n",
    "xgb1 <- xgb.train (params = params, data = dtrain, nrounds = 100, watchlist = list(val=dtest,train=dtrain), print_every_n = 10, early_stop_round = 10, maximize = F , eval_metric = \"error\")\n",
    "#model prediction\n",
    "xgbpred <- predict (xgb1,dtest)\n",
    "xgbpred <- ifelse (xgbpred > 0.2,1,0)\n",
    "\n",
    "\n",
    "#confusion matrix\n",
    "library(caret)\n",
    "confusionMatrix(xgbpred, ts_label)\n",
    "#Accuracy - 86.82%` \n",
    "\n",
    "#view variable importance plot\n",
    "mat <- xgb.importance (feature_names = colnames(mtx_train),model = xgb1)\n",
    "xgb.plot.importance (importance_matrix = mat[1:20])\n",
    "\n",
    "\n",
    "# Apply model \n",
    "dtest <- xgb.DMatrix(as.matrix(test %>% select(-order_id, -product_id)))\n",
    "test$reordered <- predict(xgb1, dtest)\n",
    "\n",
    "test$reordered <- (test$reordered > 0.2) * 1\n",
    "\n",
    "submission <- test %>%\n",
    "  filter(reordered == 1) %>%\n",
    "  group_by(order_id) %>%\n",
    "  summarise(\n",
    "    products = paste(product_id, collapse = \" \")\n",
    "  )\n",
    "\n",
    "missing <- data.frame(\n",
    "  order_id = unique(test$order_id[!test$order_id %in% submission$order_id]),\n",
    "  products = \"None\"\n",
    ")\n",
    "missing$products <- as.character(missing$products)\n",
    "submission <- submission %>% bind_rows(missing) %>% arrange(order_id)\n",
    "rownames(submission) <- submission$order_id\n",
    "write.csv(submission, file = \"sub_def.csv\", row.names = FALSE, quote = FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font color=green>\n",
    "\n",
    "Result: F1 Score of 0.3794 and position Kaggle Ranking Position 1087\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2:**\n",
    "\n",
    "As test-error did not reach convergence. We increased the number of estimators to 300 and re-run the command to get the number of optimal estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:30:39] Tree method is automatically selected to be 'approx' for faster speed. to use old behavior(exact greedy algorithm on single machine), set tree_method to 'exact'\n",
      "[13:30:43] Tree method is automatically selected to be 'approx' for faster speed. to use old behavior(exact greedy algorithm on single machine), set tree_method to 'exact'\n",
      "[13:30:47] Tree method is automatically selected to be 'approx' for faster speed. to use old behavior(exact greedy algorithm on single machine), set tree_method to 'exact'\n",
      "[13:30:52] Tree method is automatically selected to be 'approx' for faster speed. to use old behavior(exact greedy algorithm on single machine), set tree_method to 'exact'\n",
      "[13:30:56] Tree method is automatically selected to be 'approx' for faster speed. to use old behavior(exact greedy algorithm on single machine), set tree_method to 'exact'\n",
      "[1]\ttrain-error:0.092207+0.000160\ttest-error:0.092282+0.000296 \n",
      "[11]\ttrain-error:0.090597+0.000098\ttest-error:0.090685+0.000317 \n",
      "[21]\ttrain-error:0.090175+0.000098\ttest-error:0.090262+0.000330 \n",
      "[31]\ttrain-error:0.089903+0.000103\ttest-error:0.090071+0.000318 \n",
      "[41]\ttrain-error:0.089731+0.000094\ttest-error:0.089955+0.000305 \n",
      "[51]\ttrain-error:0.089608+0.000081\ttest-error:0.089873+0.000302 \n",
      "[61]\ttrain-error:0.089522+0.000060\ttest-error:0.089842+0.000318 \n",
      "[71]\ttrain-error:0.089423+0.000070\ttest-error:0.089806+0.000326 \n",
      "[81]\ttrain-error:0.089336+0.000083\ttest-error:0.089776+0.000325 \n",
      "[91]\ttrain-error:0.089255+0.000071\ttest-error:0.089735+0.000326 \n",
      "[101]\ttrain-error:0.089183+0.000069\ttest-error:0.089707+0.000309 \n",
      "[111]\ttrain-error:0.089093+0.000067\ttest-error:0.089691+0.000313 \n",
      "[121]\ttrain-error:0.089016+0.000077\ttest-error:0.089657+0.000304 \n",
      "[131]\ttrain-error:0.088938+0.000085\ttest-error:0.089633+0.000314 \n",
      "[141]\ttrain-error:0.088884+0.000090\ttest-error:0.089621+0.000306 \n",
      "[151]\ttrain-error:0.088814+0.000085\ttest-error:0.089605+0.000329 \n",
      "[161]\ttrain-error:0.088751+0.000080\ttest-error:0.089592+0.000326 \n",
      "[171]\ttrain-error:0.088683+0.000080\ttest-error:0.089558+0.000327 \n",
      "[181]\ttrain-error:0.088605+0.000086\ttest-error:0.089560+0.000335 \n",
      "[191]\ttrain-error:0.088534+0.000090\ttest-error:0.089543+0.000329 \n",
      "[201]\ttrain-error:0.088483+0.000083\ttest-error:0.089524+0.000316 \n",
      "[211]\ttrain-error:0.088414+0.000070\ttest-error:0.089523+0.000327 \n",
      "[221]\ttrain-error:0.088352+0.000070\ttest-error:0.089514+0.000334 \n",
      "[231]\ttrain-error:0.088294+0.000078\ttest-error:0.089507+0.000332 \n",
      "[241]\ttrain-error:0.088237+0.000074\ttest-error:0.089486+0.000316 \n",
      "[251]\ttrain-error:0.088186+0.000063\ttest-error:0.089480+0.000326 \n",
      "[261]\ttrain-error:0.088132+0.000074\ttest-error:0.089462+0.000319 \n",
      "[271]\ttrain-error:0.088067+0.000062\ttest-error:0.089466+0.000325 \n",
      "[281]\ttrain-error:0.088012+0.000060\ttest-error:0.089466+0.000341 \n",
      "[291]\ttrain-error:0.087958+0.000052\ttest-error:0.089455+0.000337 \n",
      "[300]\ttrain-error:0.087909+0.000057\ttest-error:0.089433+0.000326 \n"
     ]
    },
    {
     "ename": "ERROR",
     "evalue": "Error in xgb.iter.eval(bst$handle, watchlist, iteration - 1, feval): [14:09:02] amalgamation/../src/metric/elementwise_metric.cc:27: Check failed: (info.labels.size()) != (0) label set cannot be empty\n",
     "output_type": "error",
     "traceback": [
      "Error in xgb.iter.eval(bst$handle, watchlist, iteration - 1, feval): [14:09:02] amalgamation/../src/metric/elementwise_metric.cc:27: Check failed: (info.labels.size()) != (0) label set cannot be empty\nTraceback:\n",
      "1. xgb.train(params = params, data = dtrain, nrounds = 300, watchlist = list(val = dtest, \n .     train = dtrain), print_every_n = 10, early_stop_round = 10, \n .     maximize = F, eval_metric = \"error\")",
      "2. xgb.iter.eval(bst$handle, watchlist, iteration - 1, feval)"
     ]
    }
   ],
   "source": [
    "xgbcv <- xgb.cv( params = params, data = dtrain, nrounds = 300, nfold = 5, showsd = T, stratified = T, print_every_n = 10, early_stop_rounds = 20, maximize = F)\n",
    "#after 300 iterations no conversion last test-error:0.089488\n",
    "\n",
    "#300 iterations - model training\n",
    "xgb1 <- xgb.train (params = params, data = dtrain, nrounds = 300, watchlist = list(val=dtest,train=dtrain), print_every_n = 10, early_stop_round = 10, maximize = F , eval_metric = \"error\")\n",
    "#model prediction\n",
    "xgbpred <- predict (xgb1,dtest)\n",
    "xgbpred <- ifelse (xgbpred > 0.2,1,0)\n",
    "\n",
    "\n",
    "#confusion matrix\n",
    "library(caret)\n",
    "confusionMatrix(xgbpred, ts_label)\n",
    "#Accuracy - 86.86%` \n",
    "\n",
    "#view variable importance plot\n",
    "mat <- xgb.importance (feature_names = colnames(mtx_train),model = xgb1)\n",
    "xgb.plot.importance (importance_matrix = mat[1:20])\n",
    "\n",
    "\n",
    "# Apply model \n",
    "dtest <- xgb.DMatrix(as.matrix(test %>% select(-order_id, -product_id)))\n",
    "test$reordered <- predict(xgb1, dtest)\n",
    "\n",
    "test$reordered <- (test$reordered > 0.2) * 1\n",
    "\n",
    "submission <- test %>%\n",
    "  filter(reordered == 1) %>%\n",
    "  group_by(order_id) %>%\n",
    "  summarise(\n",
    "    products = paste(product_id, collapse = \" \")\n",
    "  )\n",
    "\n",
    "missing <- data.frame(\n",
    "  order_id = unique(test$order_id[!test$order_id %in% submission$order_id]),\n",
    "  products = \"None\"\n",
    ")\n",
    "missing$products <- as.character(missing$products)\n",
    "submission <- submission %>% bind_rows(missing) %>% arrange(order_id)\n",
    "rownames(submission) <- submission$order_id\n",
    "write.csv(submission, file = \"sub_def300.csv\", row.names = FALSE, quote = FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font color=green>\n",
    "\n",
    "Result: F1 Score of 0.3805265 and Kaggle Ranking Position 825\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3:**\n",
    "\n",
    "Test-error did not reach convergence, but we will continue to the next step. Which is to use gridsearch to tune max_depth and min_child_weight as tune these first will have the highest impact on model outcome. \n",
    "\n",
    "max_depth = [7,8]\n",
    "\n",
    "min_child_weight = [1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttrain-rmse:0.401068+0.000012\ttest-rmse:0.401112+0.000026 \n",
      "Multiple eval metrics are present. Will use test_rmse for early stopping.\n",
      "Will train until test_rmse hasn't improved in 10 rounds.\n",
      "\n",
      "[5]\ttrain-rmse:0.279195+0.000093\ttest-rmse:0.279420+0.000057 \n",
      "[1]\ttrain-rmse:0.401103+0.000172\ttest-rmse:0.401187+0.000105 \n",
      "Multiple eval metrics are present. Will use test_rmse for early stopping.\n",
      "Will train until test_rmse hasn't improved in 10 rounds.\n",
      "\n",
      "[5]\ttrain-rmse:0.278879+0.000166\ttest-rmse:0.279259+0.000163 \n",
      "[1]\ttrain-rmse:0.401085+0.000054\ttest-rmse:0.401123+0.000028 \n",
      "Multiple eval metrics are present. Will use test_rmse for early stopping.\n",
      "Will train until test_rmse hasn't improved in 10 rounds.\n",
      "\n",
      "[5]\ttrain-rmse:0.279200+0.000185\ttest-rmse:0.279416+0.000039 \n",
      "[1]\ttrain-rmse:0.401087+0.000004\ttest-rmse:0.401158+0.000045 \n",
      "Multiple eval metrics are present. Will use test_rmse for early stopping.\n",
      "Will train until test_rmse hasn't improved in 10 rounds.\n",
      "\n",
      "[5]\ttrain-rmse:0.278897+0.000104\ttest-rmse:0.279272+0.000016 \n",
      "[1]\ttrain-rmse:0.401106+0.000273\ttest-rmse:0.401151+0.000152 \n",
      "Multiple eval metrics are present. Will use test_rmse for early stopping.\n",
      "Will train until test_rmse hasn't improved in 10 rounds.\n",
      "\n",
      "[5]\ttrain-rmse:0.279173+0.000283\ttest-rmse:0.279387+0.000193 \n",
      "[1]\ttrain-rmse:0.401096+0.000014\ttest-rmse:0.401174+0.000067 \n",
      "Multiple eval metrics are present. Will use test_rmse for early stopping.\n",
      "Will train until test_rmse hasn't improved in 10 rounds.\n",
      "\n",
      "[5]\ttrain-rmse:0.278921+0.000010\ttest-rmse:0.279288+0.000058 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   user  system elapsed \n",
       "1649.74   31.79  297.48 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>V1</th><th scope=col>V2</th><th scope=col>V3</th><th scope=col>V4</th><th scope=col>V5</th><th scope=col>V6</th><th scope=col>V7</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>0.2794200</td><td>0.2791950</td><td>0.1      </td><td>1        </td><td>7        </td><td>0.3      </td><td>1        </td></tr>\n",
       "\t<tr><td>0.2792595</td><td>0.2788790</td><td>0.1      </td><td>1        </td><td>8        </td><td>0.3      </td><td>1        </td></tr>\n",
       "\t<tr><td>0.2794160</td><td>0.2792005</td><td>0.1      </td><td>1        </td><td>7        </td><td>0.3      </td><td>2        </td></tr>\n",
       "\t<tr><td>0.2792720</td><td>0.2788970</td><td>0.1      </td><td>1        </td><td>8        </td><td>0.3      </td><td>2        </td></tr>\n",
       "\t<tr><td>0.2793870</td><td>0.2791735</td><td>0.1      </td><td>1        </td><td>7        </td><td>0.3      </td><td>3        </td></tr>\n",
       "\t<tr><td>0.2792885</td><td>0.2789205</td><td>0.1      </td><td>1        </td><td>8        </td><td>0.3      </td><td>3        </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllllll}\n",
       " V1 & V2 & V3 & V4 & V5 & V6 & V7\\\\\n",
       "\\hline\n",
       "\t 0.2794200 & 0.2791950 & 0.1       & 1         & 7         & 0.3       & 1        \\\\\n",
       "\t 0.2792595 & 0.2788790 & 0.1       & 1         & 8         & 0.3       & 1        \\\\\n",
       "\t 0.2794160 & 0.2792005 & 0.1       & 1         & 7         & 0.3       & 2        \\\\\n",
       "\t 0.2792720 & 0.2788970 & 0.1       & 1         & 8         & 0.3       & 2        \\\\\n",
       "\t 0.2793870 & 0.2791735 & 0.1       & 1         & 7         & 0.3       & 3        \\\\\n",
       "\t 0.2792885 & 0.2789205 & 0.1       & 1         & 8         & 0.3       & 3        \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "V1 | V2 | V3 | V4 | V5 | V6 | V7 | \n",
       "|---|---|---|---|---|---|\n",
       "| 0.2794200 | 0.2791950 | 0.1       | 1         | 7         | 0.3       | 1         | \n",
       "| 0.2792595 | 0.2788790 | 0.1       | 1         | 8         | 0.3       | 1         | \n",
       "| 0.2794160 | 0.2792005 | 0.1       | 1         | 7         | 0.3       | 2         | \n",
       "| 0.2792720 | 0.2788970 | 0.1       | 1         | 8         | 0.3       | 2         | \n",
       "| 0.2793870 | 0.2791735 | 0.1       | 1         | 7         | 0.3       | 3         | \n",
       "| 0.2792885 | 0.2789205 | 0.1       | 1         | 8         | 0.3       | 3         | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  V1        V2        V3  V4 V5 V6  V7\n",
       "1 0.2794200 0.2791950 0.1 1  7  0.3 1 \n",
       "2 0.2792595 0.2788790 0.1 1  8  0.3 1 \n",
       "3 0.2794160 0.2792005 0.1 1  7  0.3 2 \n",
       "4 0.2792720 0.2788970 0.1 1  8  0.3 2 \n",
       "5 0.2793870 0.2791735 0.1 1  7  0.3 3 \n",
       "6 0.2792885 0.2789205 0.1 1  8  0.3 3 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "searchGridSubCol <- expand.grid(subsample = c(0.1), \n",
    "                                colsample_bytree = c(1),\n",
    "                                max_depth = c(7,8),\n",
    "                                min_child = seq(1,3), \n",
    "                                eta = c(0.3)\n",
    ")\n",
    "\n",
    "ntrees <- 5\n",
    "\n",
    "system.time(\n",
    "  rmseErrorsHyperparameters <- apply(searchGridSubCol, 1, function(parameterList){\n",
    "    \n",
    "    #Extract Parameters to test\n",
    "    currentSubsampleRate <- parameterList[[\"subsample\"]]\n",
    "    currentColsampleRate <- parameterList[[\"colsample_bytree\"]]\n",
    "    currentDepth <- parameterList[[\"max_depth\"]]\n",
    "    currentEta <- parameterList[[\"eta\"]]\n",
    "    currentMinChild <- parameterList[[\"min_child\"]]\n",
    "    xgboostModelCV <- xgb.cv(data =  dtrain, nrounds = ntrees, nfold = 2, showsd = TRUE, \n",
    "                             metrics = \"rmse\", verbose = TRUE, \"eval_metric\" = \"rmse\",\n",
    "                             \"objective\" = \"binary:logistic\", \"max.depth\" = currentDepth, \"eta\" = currentEta,                               \n",
    "                             \"subsample\" = currentSubsampleRate, \"colsample_bytree\" = currentColsampleRate\n",
    "                             , print_every_n = 10, \"min_child_weight\" = currentMinChild, booster = \"gbtree\",\n",
    "                             early_stopping_rounds = 10)\n",
    "    \n",
    "    xvalidationScores <- as.data.frame(xgboostModelCV$evaluation_log)\n",
    "    rmse <- tail(xvalidationScores$test_rmse_mean, 1)\n",
    "    trmse <- tail(xvalidationScores$train_rmse_mean,1)\n",
    "    output <- return(c(rmse, trmse, currentSubsampleRate, currentColsampleRate, currentDepth, currentEta, currentMinChild))\n",
    "    \n",
    "  }))\n",
    "\n",
    "output <- as.data.frame(t(rmseErrorsHyperparameters))\n",
    "head(output)\n",
    "varnames <- c(\"TestRMSE\", \"TrainRMSE\", \"SubSampRate\", \"ColSampRate\", \"Depth\", \"eta\", \"currentMinChild\")\n",
    "names(output) <- varnames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in xgb.iter.eval(bst$handle, watchlist, iteration - 1, feval): [15:23:56] amalgamation/../src/metric/elementwise_metric.cc:27: Check failed: (info.labels.size()) != (0) label set cannot be empty\n",
     "output_type": "error",
     "traceback": [
      "Error in xgb.iter.eval(bst$handle, watchlist, iteration - 1, feval): [15:23:56] amalgamation/../src/metric/elementwise_metric.cc:27: Check failed: (info.labels.size()) != (0) label set cannot be empty\nTraceback:\n",
      "1. xgb.train(params = params, data = dtrain, nrounds = 300, watchlist = list(val = dtest, \n .     train = dtrain), print_every_n = 10, early_stop_round = 10, \n .     maximize = F, eval_metric = \"error\")",
      "2. xgb.iter.eval(bst$handle, watchlist, iteration - 1, feval)"
     ]
    }
   ],
   "source": [
    "# tunning  max_depth = 8 and min_child = 1 as found in the gridseach\n",
    "\n",
    "params <- list(\n",
    "  booster = \"gbtree\", \n",
    "  objective = \"binary:logistic\",\n",
    "  eta=0.3, \n",
    "  gamma=0, \n",
    "  max_depth=8, \n",
    "  min_child_weight=1, \n",
    "  subsample=1, \n",
    "  colsample_bytree=1\n",
    ")\n",
    "\n",
    "\n",
    "#300 iterations & tunning - model training\n",
    "xgb1 <- xgb.train (params = params, data = dtrain, nrounds = 300, watchlist = list(val=dtest,train=dtrain), print_every_n = 10, early_stop_round = 10, maximize = F , eval_metric = \"error\")\n",
    "#model prediction\n",
    "xgbpred <- predict (xgb1,dtest)\n",
    "xgbpred <- ifelse (xgbpred > 0.2,1,0)\n",
    "\n",
    "\n",
    "#confusion matrix\n",
    "library(caret)\n",
    "confusionMatrix(xgbpred, ts_label)\n",
    "#Accuracy - 86.93%` \n",
    "\n",
    "#view variable importance plot\n",
    "mat <- xgb.importance (feature_names = colnames(mtx_train),model = xgb1)\n",
    "xgb.plot.importance (importance_matrix = mat[1:20])\n",
    "\n",
    "\n",
    "# Apply model \n",
    "dtest <- xgb.DMatrix(as.matrix(test %>% select(-order_id, -product_id)))\n",
    "test$reordered <- predict(xgb1, dtest)\n",
    "\n",
    "test$reordered <- (test$reordered > 0.2) * 1\n",
    "\n",
    "submission <- test %>%\n",
    "  filter(reordered == 1) %>%\n",
    "  group_by(order_id) %>%\n",
    "  summarise(\n",
    "    products = paste(product_id, collapse = \" \")\n",
    "  )\n",
    "\n",
    "missing <- data.frame(\n",
    "  order_id = unique(test$order_id[!test$order_id %in% submission$order_id]),\n",
    "  products = \"None\"\n",
    ")\n",
    "missing$products <- as.character(missing$products)\n",
    "submission <- submission %>% bind_rows(missing) %>% arrange(order_id)\n",
    "rownames(submission) <- submission$order_id\n",
    "write.csv(submission, file = \"sub_def300tun.csv\", row.names = FALSE, quote = FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font color=green>\n",
    "\n",
    "Result: F1 Score of 0.3764418 and Kaggle Ranking Position 1618\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
